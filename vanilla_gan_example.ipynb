{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Application in Video Generation Example\n",
    "A General Adversarial Network is trained to generate short 64 frame videos (gifs). In this example, the network is trained on a single video (64 frames) to illustrate its functionality. Thus, the network is overfit. The next step is to train the network on 2-3 minute videos and produce gifs based on input images (i.e. predicting future frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load '/Users/jesusnavarro/Desktop/gan_video/core/main.py'\n",
    "\n",
    "# import dependencies\n",
    "from keras_preprocessing.image import save_img\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.autograd\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import logging\n",
    "from core.model import *\n",
    "from core.UAVDataset import *\n",
    "from examples.Logger import Logger\n",
    "import sys\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "from array2gif import write_gif\n",
    "\n",
    "\n",
    "\n",
    "def make_gif(images, filename):\n",
    "    ''' Helper method to create gif from images\n",
    "    \n",
    "    @param: images - tensor image of size (height, width, channels)\n",
    "    @param: filename - str path to save gif\n",
    "    '''\n",
    "    x = images.permute(1,2,3,0)\n",
    "    x = x.numpy()\n",
    "    x = img_as_ubyte(x)\n",
    "    frames = []\n",
    "    for i in range(32):\n",
    "        frames += [x[i]]\n",
    "    imageio.mimsave(filename, frames, duration=0.1)\n",
    "\n",
    "def denorm(x):\n",
    "    ''' Helper method to denormalize the images\n",
    "    \n",
    "    @param x: tensor image (height, width, channels)\n",
    "    '''\n",
    "    out = (x + 1.0) / 2.0\n",
    "    tf = nn.Tanh()\n",
    "    return tf(out)\n",
    "\n",
    "\n",
    "def setup_logger(logger_name):\n",
    "    # format string\n",
    "    LOG_FORMAT = \"%(levelname)s - %(asctime)s - %(messages)s\"\n",
    "    logger = logging.getLogger(logger_name)\n",
    "\n",
    "    # create formatter\n",
    "    formatter = logging.Formatter(fmt=LOG_FORMAT)\n",
    "\n",
    "    # create handler, streamhandler, and format\n",
    "    file_handler = logging.FileHandler('training_log.txt', mode='w')\n",
    "    file_handler.setFormatter(file_handler)\n",
    "    screen_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "    screen_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(screen_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def save_image(img, filename):\n",
    "    ''' Saves img to filename\n",
    "    '''\n",
    "    \n",
    "    img = img.detach()\n",
    "    pil_img = ToPILImage()\n",
    "    img = pil_img(img)\n",
    "    img.save(filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Visualize Sample Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose transforms to apply to the data\n",
    "composed = transforms.Compose([Rescale(64), ToTensor()])\n",
    "\n",
    "# Define location of data pickle files\n",
    "# This example uses 32 boat9 image frames of UAV dataset 20 batches\n",
    "working_dir = '/Users/jesusnavarro/Desktop/gan_video'\n",
    "sub_dir = '/pickle_data/boat9_video_test_20_batches.pickle'\n",
    "\n",
    "# Instantiate DataLoader object \n",
    "transformed_uav_dataset = UAVDataset(pickle_path=working_dir + sub_dir,\n",
    "                                     transform=composed)\n",
    "\n",
    "data_loader = DataLoader(transformed_uav_dataset,\n",
    "                         batch_size=32,\n",
    "                         shuffle=False)\n",
    "\n",
    "# Save gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = video_list[0][0].permute(1, 0, 2, 3, 4).numpy()[0]\n",
    "print(nd)\n",
    "write_gif(np.array(nd), 'sample_gif.gif', fps=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Generator and Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "d_loss, g_loss, d_real_output, d_fake_output = 0, 0, 0, 0\n",
    "l1_lambda = 10\n",
    "num_epochs = 120\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Instantiate generator and discriminator\n",
    "generator = VideoGen().float()\n",
    "discriminator = VideoDiscriminator().float()\n",
    "\n",
    "# Directory to save generator videos\n",
    "DIR_TO_SAVE = \"./gen_videos/\"\n",
    "if not os.path.exists(DIR_TO_SAVE):\n",
    "    os.makedirs(DIR_TO_SAVE)\n",
    "sample_input = None\n",
    "sample_input_set = False\n",
    "\n",
    "# Define optimizer for the discriminator and generator\n",
    "\n",
    "#d_optimizer = optim.RMSprop(discriminator.parameters(), lr=5e-5)\n",
    "#g_optimizer = optim.RMSprop(generator.parameters(), lr=5e-5)\n",
    "\n",
    "# Adam optimizer\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.555, 0.5))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.555, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(model_name='VGAN', data_name='boat9') # Used only to display updates \n",
    "for epoch in range(num_epochs):\n",
    "    save = True # boolean to save gif\n",
    "    for (iter_num, batch) in enumerate(data_loader, 1):\n",
    "        \n",
    "        real_video = batch['video'][0]\n",
    "        filenames = batch['video'][1]\n",
    "        real_video = torch.squeeze(real_video, dim=1).unsqueeze(0)\n",
    "        \n",
    "        # reset gradients\n",
    "        generator.zero_grad()\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        N = real_video.size(0)\n",
    "\n",
    "        real_labels = torch.LongTensor(np.ones(N, dtype=int))\n",
    "        fake_labels = torch.LongTensor(np.zeros(N,  dtype=int))\n",
    "\n",
    "\n",
    "        # Train discriminator\n",
    "        d_real_output_sig, d_real_output = discriminator(real_video.permute(0, 2, 1, 3, 4).float())\n",
    "        d_real_output_sig, d_real_output = d_real_output_sig.squeeze(), d_real_output.squeeze()\n",
    "\n",
    "        # Generate a fake video, detach parameters\n",
    "        latent_z = torch.randn(N, 100, 1, 1, 1)\n",
    "        fake_video = generator(latent_z).detach()\n",
    "        \n",
    "        # Discriminator outputs sigmoid output and raw output (WGAN/Vanilla GAN)\n",
    "        d_fake_output_sig, d_fake_output = discriminator(fake_video)\n",
    "\n",
    "        # Compute real and fake loss\n",
    "        if N == 1:\n",
    "            d_fake_output = d_fake_output.squeeze().unsqueeze(0)\n",
    "            d_real_output = d_real_output.squeeze().unsqueeze(0)\n",
    "        else:\n",
    "            d_fake_output = d_fake_output.squeeze()\n",
    "            d_real_output = d_real_output.squeeze()\n",
    "\n",
    "        d_loss = -(torch.mean(d_real_output) - torch.mean(d_fake_output))\n",
    "\n",
    "        # Update Gradient\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        for p in discriminator.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "\n",
    "        if iter_num % 2 == 0:  # Train generator\n",
    "            first_frame = real_video[0:1, 0:1, :, :, :].squeeze()\n",
    "            latent_z = torch.randn(N, 100, 1, 1, 1)\n",
    "            fake_videos = generator(latent_z)\n",
    "            d_fake_outputs_sig, d_fake_outputs = discriminator(fake_videos)\n",
    "            d_fake_outputs_sig, d_fake_outputs = d_fake_outputs_sig.squeeze(), d_fake_outputs.squeeze()\n",
    "            gen_first_frame = fake_videos[0:1, :, 0:1, :, :].squeeze()\n",
    "            reg_loss = torch.mean(torch.abs(real_video.float() - fake_videos.permute(0,2,1,3,4).float())) * l1_lambda\n",
    "            g_loss = -torch.mean(d_fake_outputs) + reg_loss\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "        if iter_num % 5 == 0:\n",
    "            logger.display_status(\n",
    "                epoch, num_epochs, iter_num, len(data_loader),\n",
    "                d_loss, g_loss, d_real_output, d_fake_output\n",
    "            )\n",
    "\n",
    "            if (epoch + 1) % 10 == 0 and save:\n",
    "                # Generate video to save\n",
    "                sample_input = torch.randn(N, 100, 1, 1, 1)\n",
    "                gen_out = generator(sample_input)\n",
    "                \n",
    "                # Save video from generator\n",
    "                make_gif(denorm(gen_out.data.cpu()[0]),\n",
    "                         DIR_TO_SAVE + 'fake_gifs_sample_it%s_epoch%s.gif' % (iter_num, epoch))\n",
    "                save = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Training Improvement\n",
    "\n",
    "The gifs are displayed below for epochs 10, 40, 60, and 120. Recall that the GAN performs well since there is overfitting. Only a single video is being used to train the GAN for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gifs = [\"./gen_videos/fake_gifs_sample_it5_epoch9.gif\",\n",
    "       \"./gen_videos/fake_gifs_sample_it5_epoch39.gif\",\n",
    "       \"./gen_videos/fake_gifs_sample_it5_epoch59.gif\",\n",
    "       \"./gen_videos/fake_gifs_sample_it5_epoch119.gif\"]\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real video (compressed)\n",
    "# Images were normalized\n",
    "make_gif(real_video.squeeze().permute(1, 0, 2, 3), DIR_TO_SAVE + 'real_video.gif')\n",
    "Image(filename= DIR_TO_SAVE + 'real_video.gif', width=250, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 10\n",
    "Image(filename=gifs[0] , width=250, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 40\n",
    "Image(filename=gifs[1] , width=250, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 60\n",
    "Image(filename=gifs[2] , width=250, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 120\n",
    "Image(filename=gifs[3] , width=250, height=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
